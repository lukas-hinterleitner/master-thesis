pyxis: importing docker image: lukashinterleitner/master-thesis-data-science:latest
pyxis: imported docker image: lukashinterleitner/master-thesis-data-science:latest
[2025-05-19 23:45:29,584] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /srv/home/users/a51912219cs/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Setting: paraphrased
Computation Type: gradient-similarity
Use Random Projection: True
Device: cuda:0
Model parameters: 1176764416
======================
OlmoForCausalLM(
  (model): OlmoModel(
    (embed_tokens): Embedding(50304, 2048, padding_idx=1)
    (layers): ModuleList(
      (0-15): 16 x OlmoDecoderLayer(
        (self_attn): OlmoAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        )
        (mlp): OlmoMLP(
          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): OlmoLayerNorm()
        (post_attention_layernorm): OlmoLayerNorm()
      )
    )
    (norm): OlmoLayerNorm()
    (rotary_emb): OlmoRotaryEmbedding()
  )
  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)
)
Calculating gradients + similarities using random projection:   0%|          | 0/988 [00:00<?, ?it/s]P(lima_0) vs O(lima_0):   0%|          | 0/988 [00:02<?, ?it/s]                                      P(lima_0) vs O(lima_0):   0%|          | 0/988 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/app/main.py", line 184, in <module>
    main()
  File "/app/main.py", line 125, in main
    gradient_similarities = calculate_paraphrased_random_projected_gradient_similarities(
  File "/app/application/computation.py", line 234, in calculate_paraphrased_random_projected_gradient_similarities
    down_projected_original_layer_gradient = projector.project(
  File "/usr/local/lib/python3.10/dist-packages/trak/projectors.py", line 421, in project
    raise e
  File "/usr/local/lib/python3.10/dist-packages/trak/projectors.py", line 408, in project
    result = fn(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 202.64 GiB. GPU 0 has a total capacity of 79.11 GiB of which 69.06 GiB is free. Including non-PyTorch memory, this process has 10.04 GiB memory in use. Of the allocated memory 9.02 GiB is allocated by PyTorch, and 305.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
