pyxis: importing docker image: lukashinterleitner/master-thesis-data-science:latest
pyxis: imported docker image: lukashinterleitner/master-thesis-data-science:latest
[2025-05-21 11:23:38,284] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /srv/home/users/a51912219cs/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Setting: paraphrased
Computation Type: gradient-similarity
Use Random Projection: True
Device: cuda:0
Model parameters: 1176764416
======================
OlmoForCausalLM(
  (model): OlmoModel(
    (embed_tokens): Embedding(50304, 2048, padding_idx=1)
    (layers): ModuleList(
      (0-15): 16 x OlmoDecoderLayer(
        (self_attn): OlmoAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        )
        (mlp): OlmoMLP(
          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): OlmoLayerNorm()
        (post_attention_layernorm): OlmoLayerNorm()
      )
    )
    (norm): OlmoLayerNorm()
    (rotary_emb): OlmoRotaryEmbedding()
  )
  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)
)
Calculating gradients + similarities using random projection:   0%|          | 0/988 [00:00<?, ?it/s]Calculating gradients + similarities using random projection:   0%|          | 0/988 [00:03<?, ?it/s]
Projection dimension: 11767644
Paraphrased gradient shape: torch.Size([1176764416])
Paraphrased gradient reshaped shape: torch.Size([1, 1176764416])
Traceback (most recent call last):
  File "/app/main.py", line 184, in <module>
    main()
  File "/app/main.py", line 125, in main
    gradient_similarities = calculate_paraphrased_random_projected_gradient_similarities(
  File "/app/application/computation.py", line 232, in calculate_paraphrased_random_projected_gradient_similarities
    down_projected_paraphrased_gradient = projector.project(
  File "/usr/local/lib/python3.10/dist-packages/trak/projectors.py", line 421, in project
    raise e
  File "/usr/local/lib/python3.10/dist-packages/trak/projectors.py", line 408, in project
    result = fn(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.06 GiB. GPU 0 has a total capacity of 15.77 GiB of which 4.29 GiB is free. Including non-PyTorch memory, this process has 11.47 GiB memory in use. Of the allocated memory 10.98 GiB is allocated by PyTorch, and 125.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
